---
title: "Census Data Exploratory Analysis"
output:
  html_notebook: default
  html_document: default
---

## Overview

Predict whether income exceeds \$50K/yr based on census data. Also known as "Adult" dataset. Extraction was done by Barry Becker from the 1994 Census database. Prediction task is to determine whether a person makes over 50K a year. See the [data source](https://archive.ics.uci.edu/ml/datasets/Census+Income) and [description](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names) for more information. These data are also used for demonstrating [Tensorflow](https://www.tensorflow.org/tutorials/wide).

## Exploratory data analysis

The biggest drivers for predicting income over \$50k are: marital status (married is better), education (more is better), and sex (male is better). We will explore the continuous and categorical predictors before building statistical models. Data manipulation is carried out in `dplyr` and visualizations are done in `ggplot2` and `plotly`.

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
library(plotly)
library(pROC)
library(glmnet)

```

## Download and read the data

The data can be downloaded from the web. The training and test data are 3.8 MB and 1.9 MB respectively. The missing values are converted from `?` to `NA`.

```{r, eval=FALSE, message=FALSE, warning=FALSE}
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
              "data/train_raw.csv")
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test", 
              "data/test_raw.csv")
```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
col_names = c(
  "age", "workclass", "fnlwgt", "education", "education_num",
  "marital_status", "occupation", "relationship", "race", "gender",
  "capital_gain", "capital_loss", "hours_per_week", "native_country",
  "income_bracket"
)

train_raw <- read_csv("data/train_raw.csv", col_names = col_names, na = "?")
test_raw  <- read_csv("data/test_raw.csv", col_names = col_names, na = "?", skip = 1)
```

## Create modeling data

Convert the target variable `income_bracket` into a numeric value. Create a new column `age_buckets` and remove records with missing values. Apply to both the test and training data. Create interactions if desired. Note: These interactions can be extremely time consuming to model, therefore they are examined here, but are not included in the predictive models.

```{r}
format.rawdata <- function(data){
  data %>%
    mutate(label = ifelse(income_bracket == ">50K" | income_bracket == ">50K.", 1, 0)) %>%
    mutate(age_buckets = cut(age, c(16, 18, 25, 30, 35, 40, 45, 50, 55, 60, 65, 90))) %>%
    select(label, gender, native_country, education, education_num, occupation, workclass, marital_status, 
           race, age_buckets) %>%
    na.omit
}

train <- train_raw %>% format.rawdata
test  <- test_raw  %>% format.rawdata
```

## Plot categorical columns

Most of the columns in the census data are categorical. We plot a few of the most important columns here. The complete list of categorical columns are:

* workclass
* education
* marital_status
* occupation
* relationship
* race
* gender
* native_country

```{r}
plot.main.effects <- function(data, x, y){
  data %>%
    mutate_(group = x, metric = y) %>%
    group_by(group) %>%
    summarize(percent = 100 * mean(metric)) %>%
    ggplot(aes(x = reorder(group, percent), percent)) +
    geom_bar(stat="identity", fill = "lightblue4") +
    coord_flip() +
    labs(y = "Percent", x = "") +
    ggtitle(paste("Percent surveyed with incomes over $50k by", x))
}

plot.main.effects(train, "marital_status", "label")
plot.main.effects(train, "gender", "label")
plot.main.effects(train, "education", "label")
```

## Plot continuous columns

We can compare the distribution of the categorical variables for those who earn more than \$50k and those who earn less. The complete list of categorical variables are:

* age
* education_num
* capital_gain
* capital_loss
* hours_per_week

```{r}
plot.continuous <- function(data, x, y, alpha = 0.2, ...){ 
  lab <- stringr::str_replace_all(y, "_", " ") %>% stringr::str_to_title(y)
  data %>%
    select_(groups = x, y = y) %>%
    na.omit %>%
    ggplot(aes(y, fill = groups)) + geom_density(alpha = alpha, ...) +
    labs(x = lab, y = "") +
    ggtitle(paste0("Income by ", lab))
}

# People who earn more also work more, are better educated, and are older
plot.continuous(train_raw, "income_bracket", "age")
plot.continuous(train_raw, "income_bracket", "education_num", adjust = 5)
plot.continuous(train_raw, "income_bracket", "hours_per_week", adjust = 5)

```


## Plot interactions

We can examine some two-way and three-way intearcations with choropleth maps:

```{r}
p <- train %>%
  select(education_num, age_buckets, label) %>%
  group_by(age_buckets, education_num) %>%
  summarize(percent = 100 * mean(label)) %>%
  ggplot(aes(education_num, age_buckets, fill = percent)) +
  geom_tile() +
  labs(x = "Education", y = "Age") +
  ggtitle("Percent surveyed with incomes over $50k by age, education")
ggplotly(p)

p <- train %>%
  select(age_buckets, education_num, occupation, label) %>%
  group_by(age_buckets, education_num, occupation) %>%
  summarize(percent = 100 * mean(label)) %>%
  ggplot(aes(education_num, age_buckets, fill = percent)) +
  geom_tile() +
  facet_wrap( ~ occupation) +
  labs(x = "Education", y = "Age") +
  ggtitle("Percent surveyed with incomes over $50k by age, education, and occupation")
ggplotly(p)

```

## Save Trained Data for Analysis

```{r}
write_csv(train, "data/train.csv")
write_csv(test, "data/test.csv")
```


# Logistic Regression

The logistic model uses main effects only against the training data. No regularization is applied. We assess the model fit with a hold out sample. We can build logistic models with the `stats` package.

## Train Model

Gender, education, and marital status are all highly significant. Marrital status in particular is a good predictor of those earning more than $50k.

```{r}
m1 <- glm(label ~ gender + native_country + education + occupation + workclass + marital_status +  
         race + age_buckets, binomial, train)
summary(m1)
#anova(m1) # takes a while to run
#plot(m1) # legacy plots not that useful
```

## Predict

The high area under the curve (AUC) of 0.883 indicator that this model might be overfitting. The lift chart shows that 80% of those in the uppper decile earn more than `$50k`, compared to a tiny fraction in the lower decile.

```{r}
# Predict
pred <- bind_rows("train" = train, "test" = test, .id = "data") %>%
  mutate(pred = predict(m1, ., type = "response")) %>%
  mutate(decile = ntile(desc(pred), 10)) %>%
  select(data, label, pred, decile)

# ROC plot
pred %>%
  filter(data == "test") %>%
  roc(label ~ pred, .) %>%
  plot.roc(., print.auc = TRUE)

# Lift plot
pred %>%
  group_by(data, decile) %>%
  summarize(percent = 100 * mean(label)) %>%
  ggplot(aes(decile, percent, fill = data)) + geom_bar(stat = "Identity", position = "dodge") +
  ggtitle("Lift chart for logistic regression model")
```


***

# Elastic net

The elastic net is a regularized regression method that uses L1 (lasso) and L2 (ridge) penalties. We can build elastic net models with the `glmnet` package.

## Train model

Whereas the logistic method used formulas, the elastic net model requires us to construct a model matrix from the categorical predictors. We then attempt to choose a value `lambda` that optimizes the L1 and L2 penalties. We can examine various predictor sets for different values of `lambda`. Optionally, we can use cross validation to programmatically determine the best choice of `lambda`.

```{r}
# Convert to factors
alldata <- bind_rows("train" = train, "test" = test, .id = "data") %>%
  select(-education_num) %>%
  mutate_each(., funs(factor(.))) %>%
  model.matrix( ~ ., .)

# Create training prediction matrix
train.factors <- list(x = alldata[alldata[,'datatrain'] == 1, -(1:3)],
                     y = alldata[alldata[,'datatrain'] == 1, 3])

# Create test prediction matrix
test.factors <- list(x = alldata[alldata[,'datatrain'] == 0, -(1:3)],
                    y = alldata[alldata[,'datatrain'] == 0, 3])

# Fit a regularized model
fit1 <- glmnet(train.factors$x, train.factors$y, family = "binomial")
plot(fit1)
print(fit1)
(m2 <- coef.glmnet(fit1, s = 0.02)) # extract coefficients at a single value of lambda
```

```{r, eval=FALSE}
# Cross validation (long running for full dataset)
cvfit <- cv.glmnet(train.factors$x, train.factors$y, family = "binomial", type.measure = "class")
plot(cvfit)
cvfit$lambda.min # 0.0001971255
```

## Predict

Once you have chosen a value for `lambda` you can score the test set and examine the ROC and lift charts. This model has a slightly smaller AUC and lift values, but the overall results look very similar to logistic regression.

```{r}
# Predict and plot the AUC
test.factors$pred <- predict(fit1, test.factors$x, s=0.02, type = "response") # make predictions
data.frame(resp = test.factors$y, pred = c(test.factors$pred)) %>%
  roc(resp ~ pred, .) %>%
  plot.roc(., print.auc = TRUE)

# Lift chart
data.frame(data = ifelse(alldata[, 'datatrain'], "train", "test"),
           label = alldata[,'label1'],
           pred = c(predict.glmnet(fit1, alldata[, -(1:3)], s=0.02))) %>%
  mutate(decile = ntile(desc(pred), 10)) %>%
  group_by(data, decile) %>%
  summarize(percent = 100 * mean(label)) %>%
  ggplot(aes(decile, percent, fill = data)) + geom_bar(stat = "Identity", position = "dodge") +
  ggtitle("Lift chart for elastic net model")
```

# Save

Finally, save the predicted output and the model for building apps.

```{r}
# Score predictions
pred.out <- test %>%
  mutate(pred.glm = pred$pred[pred$data == "test"]) %>%
  mutate(pred.net = c(test.factors$pred)) %>%
  mutate(income_bracket = ifelse(label, ">50K", "<=50K")
)

# Output predictions to file
write_csv(pred.out, "data/pred.csv")
saveRDS(m1, file = "data/logisticModel.rds")
saveRDS(m2, file = "data/elasticnetModel.rds")
```

***

# Caret

If you want to try other models, take a look at the `caret` package. The `caret` package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

as well as other functionality. See the [caret documentation](http://topepo.github.io/caret/index.html) for more details.

```{r, eval=FALSE}
library(caret)
library(e1071)
library(gbm)

## convert label to factor
train$y <- factor(train$label)

## Cross validation
fitControl <- trainControl(method = "cv", number = 3, repeats = 1)

## Fit a gbm model with cross validation (this will take a long time!)
gbmFit1 <- train(y ~ gender + education + occupation + workclass + marital_status + age_buckets, 
                 data = train, 
                 method = "gbm", 
                 trControl = fitControl,
                 verbose = FALSE)

## Summarize
summary(gbmFit1)
```



