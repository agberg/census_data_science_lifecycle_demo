---
title: "Predicting Who Will Make More Than $50k"
output:
  html_notebook:
    code_folding: hide
---

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
library(plotly)
library(pROC)
library(glmnet)
library(broom)


```

## Overview

Predict whether income exceeds \$50K/yr based on census data. Also known as "Adult" dataset. Extraction was done by Barry Becker from the 1994 Census database. Prediction task is to determine whether a person makes over 50K a year. See the [data source](https://archive.ics.uci.edu/ml/datasets/Census+Income) and [description](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names) for more information.

## Download, Read, and Preprocess Data

The focus of this analysis is not the data ingesting and cleaning process.  Details are available in the code below.  For more information on data munging practices, see [here](http://r4ds.had.co.nz/wrangle-intro.html).

Once the data is pre-processed the training and testing sets are saved as CSV files for use in this analysis and in applications for delivery.

```{r, eval=FALSE, message=FALSE, warning=FALSE}

# The data can be downloaded from the web. The training and test data are 3.8 MB and 1.9 MB respectively. The missing values are converted from `?` to `NA`.

download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
              "data/train_raw.csv")
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test", 
              "data/test_raw.csv")

```

```{r}

# Read in and save the data

col_names = c(
  "age", "workclass", "fnlwgt", "education", "education_num",
  "marital_status", "occupation", "relationship", "race", "gender",
  "capital_gain", "capital_loss", "hours_per_week", "native_country",
  "income_bracket"
)

train_raw <- read_csv("data/train_raw.csv", col_names = col_names, na = "?")
test_raw  <- read_csv("data/test_raw.csv", col_names = col_names, na = "?", skip = 1)

# Create modeling data

# Convert the target variable `income_bracket` into a numeric value. Create a new column `age_buckets` and remove records with missing values. Apply to both the test and training data. Create interactions if desired. Note: These interactions can be extremely time consuming to model, therefore they are examined here, but are not included in the predictive models.


format.rawdata <- function(data){
  data %>%
    mutate(label = ifelse(income_bracket == ">50K" | income_bracket == ">50K.", 1, 0)) %>%
    mutate(age_buckets = cut(age, c(16, 18, 25, 30, 35, 40, 45, 50, 55, 60, 65, 90))) %>%
    select(label, gender, native_country, education, education_num, occupation, workclass, marital_status, 
           race, age_buckets) %>%
    na.omit
}

train <- train_raw %>% format.rawdata
test  <- test_raw  %>% format.rawdata

# Save Trained Data for Analysis

write_csv(train, "data/train.csv")
write_csv(test, "data/test.csv")


```

## Exploratory data analysis

The biggest drivers for predicting income over \$50k are: marital status (married is better), education (more is better), and sex (male is better). We will explore the continuous and categorical predictors before building statistical models. Data manipulation is carried out in `dplyr` and visualizations are done in `ggplot2` and `plotly`.

### Plot categorical columns

Most of the columns in the census data are categorical. We plot a few of the most important columns here. The complete list of categorical columns are:

* workclass
* education
* marital_status
* occupation
* relationship
* race
* gender
* native_country

```{r}





plot.main.effects <- function(data, x, y){
  data %>%
    mutate_(group = x, metric = y) %>%
    group_by(group) %>%
    summarize(percent = 100 * mean(metric)) %>%
    ggplot(aes(x = reorder(group, percent), percent)) +
    geom_bar(stat="identity", fill = "lightblue4") +
    coord_flip() +
    labs(y = "Percent", x = "") +
    ggtitle(paste("Percent surveyed with incomes over $50k by", x))
}

plot.main.effects(train, "marital_status", "label")
plot.main.effects(train, "gender", "label")
plot.main.effects(train, "education", "label")
```

### Plot continuous columns

We can compare the distribution of the categorical variables for those who earn more than \$50k and those who earn less. The complete list of categorical variables are:

* age
* education_num
* capital_gain
* capital_loss
* hours_per_week

```{r}
plot.continuous <- function(data, x, y, alpha = 0.2, ...){ 
  lab <- stringr::str_replace_all(y, "_", " ") %>% stringr::str_to_title(y)
  data %>%
    select_(groups = x, y = y) %>%
    na.omit %>%
    ggplot(aes(y, fill = groups)) + geom_density(alpha = alpha, ...) +
    labs(x = lab, y = "") +
    ggtitle(paste0("Income by ", lab))
}

# People who earn more also work more, are better educated, and are older
plot.continuous(train_raw, "income_bracket", "age")
plot.continuous(train_raw, "income_bracket", "education_num", adjust = 5)

```


### Plot interactions

We can examine some two-way and three-way intearcations with choropleth maps:

```{r}
p <- train %>%
  select(education_num, age_buckets, label) %>%
  group_by(age_buckets, education_num) %>%
  summarize(percent = 100 * mean(label)) %>%
  ggplot(aes(education_num, age_buckets, fill = percent)) +
  geom_tile() +
  labs(x = "Education", y = "Age") +
  ggtitle("Percent surveyed with incomes over $50k by age, education")
ggplotly(p)


```




# Logistic Regression

The logistic model uses main effects only against the training data. No regularization is applied. We assess the model fit with a hold out sample. We can build logistic models with the `stats` package.

## Motivation

After considering a variety of options for the model, we select Logistic Regression because it fits the data set well and is easily interpretable.  It works in many contexts, and it is possible to expose some feature selection to the end user without running too closely into the realm of over-fitting.

Certainly this is not the most sophisticated modeling type, but it is a good benchmark for more complicated analyses.  It serves as a good benchmark for discussions with clients and for rapid prototyping/evaluation of hypotheses. 

## Train Model

Gender, education, and marital status are all highly significant. Marrital status in particular is a good predictor of those earning more than $50k.

```{r}
m1 <- glm(label ~ gender + native_country + education + occupation + workclass + marital_status +  
         race + age_buckets, binomial, train)
summary(m1)
#anova(m1) # takes a while to run
#plot(m1) # legacy plots not that useful


```

Examine all statistically significant indicators (p.value < .05) in descending order of the absolute value of the coefficient.

```{r}

tidy(m1) %>% 
  filter(p.value < .05) %>% 
  arrange(desc(abs(estimate)))


```

## Predict

The high area under the curve (AUC) of 0.883 indicator that this model might be overfitting. The lift chart shows that 80% of those in the uppper decile earn more than `$50k`, compared to a tiny fraction in the lower decile.

```{r}
# Predict
pred <- bind_rows("train" = train, "test" = test, .id = "data") %>%
  mutate(pred = predict(m1, ., type = "response")) %>%
  mutate(decile = ntile(desc(pred), 10)) %>%
  select(data, label, pred, decile)

# ROC plot
pred %>%
  filter(data == "test") %>%
  roc(label ~ pred, .) %>%
  plot.roc(., print.auc = TRUE)

# Lift plot
pred %>%
  group_by(data, decile) %>%
  summarize(percent = 100 * mean(label)) %>%
  ggplot(aes(decile, percent, fill = data)) + geom_bar(stat = "Identity", position = "dodge") +
  ggtitle("Lift chart for logistic regression model")
```


## Iterate Feature Selection

- Given that the AOC on the test set was so significant, we adjust the feature selection to mitigate the likely over-fitting.

- (I'm not sure what the best tools to apply would be here, but I think that it's important to have something)

## Conclusions

### Main Context-specific Points

### Methodology and Learning

#### Overfitting - detecting it, avoiding it

#### Feature Selection

#### Issues with Logistic Regression

#### See Also


